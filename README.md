DÆ°á»›i Ä‘Ã¢y lÃ  báº£n hoÃ n chá»‰nh Ä‘á»ƒ báº¡n **copyâ€“paste trá»±c tiáº¿p vÃ o `README.md`**:

---

# Everbot Offline RAG (Law PDF)

This project implements an **offline RAG pipeline** over a law document PDF
(example: `ä¸­è¯æ°‘åœ‹æ†²æ³•.pdf`).

---

# ğŸ“Œ Data & Deliverables

## ğŸ“„ Data Source

The **PDF file (e.g., `ä¸­è¯æ°‘åœ‹æ†²æ³•.pdf`) is the ONLY data source** used by the system.

All downstream artifacts are generated strictly from this PDF:

* `articles_extracted.jsonl`
* `law_chunks.jsonl`
* `law.index`
* `law_ontology.json`

No external legal dataset is used at any stage.

---

## ğŸ“˜ Handover Document

The Word file:

```
RAG_Offline_Handover_EN.docx
```

is the **project handover documentation**.

It contains:

* System design explanation
* Data processing pipeline description
* Retrieval and re-ranking strategy
* Prompt construction logic
* Token limits and truncation policy
* Model selection rationale
* Resource constraints and optimization considerations

âš ï¸ The Word file is **NOT used as input data** to the RAG system.
It is strictly documentation for review and evaluation purposes.

---

# ğŸ¯ Design Philosophy (Audit-Friendly)

This project is intentionally structured to be review-safe and transparent:

* PDF â†’ extracted articles â†’ indexed
* Clear intermediate artifact: `articles_extracted.jsonl`
* Fully reproducible pipeline
* No hidden external knowledge sources

Reviewers can verify that:

* The system processes the PDF directly
* Article segmentation is transparent
* The FAISS index is built only from extracted content
* All retrieval results are grounded in the PDF-derived articles

---

# ğŸ—‚ Folder Structure

```
everbot_rag_refactor/
â”œâ”€ rag_ollama_min.py
â”œâ”€ RAG_Offline_Handover_EN.docx     # ğŸ“˜ Handover document
â””â”€ everbot_rag/
   â”œâ”€ cli.py
   â”œâ”€ config.py
   â”œâ”€ pdf_extract.py
   â”œâ”€ index_build.py
   â”œâ”€ ontology.py
   â”œâ”€ retriever.py
   â”œâ”€ reranker.py
   â”œâ”€ qa.py
   â”œâ”€ pipeline.py
   â””â”€ utils_text.py
```

---

# ğŸ”„ Build Pipeline

## Step 1 â€” Extract from PDF

```
PDF
 â†’ text (pdfplumber)
 â†’ clean
 â†’ split into Article blocks (ç¬¬Xæ¢)
 â†’ articles_extracted.jsonl
```

`articles_extracted.jsonl` is an **intermediate audit artifact generated from the PDF**.

It exists to improve:

* Debuggability
* Transparency
* Reproducibility
* Review safety

---

## Step 2 â€” Build Index

```
articles_extracted.jsonl
 â†’ FAISS index
 â†’ metadata file
 â†’ ontology (auto-generated keywords for ALL articles)
```

The index is built strictly from the JSONL file, which itself was generated from the PDF.

---

# ğŸš€ Usage

## Build

```bash
python rag_ollama_min.py build --pdf "ä¸­è¯æ°‘åœ‹æ†²æ³•.pdf"
```

Optional build-time enhancements:

```bash
--llm-concepts     # Generate per-article concept aliases
--llm-synonyms     # Expand synonym table
```

---

## Ask

```bash
python rag_ollama_min.py ask "æ ¹æ“šæ†²æ³•ç¬¬20æ¢çš„è¦å®šï¼Œåœ‹æ°‘æœ‰ä»€éº¼æ¨£çš„ç¾©å‹™ï¼Ÿ" --topk 5
```

Output includes:

* Answer in Traditional Chinese
* English translation (Engsub)
* Extracted Articles
* Evidence in both languages

If insufficient evidence:

```
Cannot answer the question based on the extracted constitutional articles.
```

---

# ğŸ§  Retrieval Strategy (High-Level)

1. Normalize the question to Traditional Chinese
2. Expand via ontology (seed synonyms + optional LLM concepts)
3. Keyword-first retrieval across all articles
4. Vector assist (FAISS cosine similarity)
5. Cheap LLM re-ranking (top-N)
6. Strictly grounded answer generation
7. English translation

---

# âš–ï¸ Important Notes

* The PDF is the **data source**.
* The Word file is the **handover documentation**.
* All index and ontology files are generated artifacts.
* The system does not use any external legal dataset.
* All answers are grounded strictly in extracted constitutional articles.
